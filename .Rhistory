p_sim
U <- cbind(X, W)
# profvis::profvis(
system.time(
dat_mini <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
learn_rate = 0.1, num_iters = 2000,
print_every = 50,
minibatch_size = 2^8)
)
# Using full gradient
# profvis::profvis(
system.time(
dat_full <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
learn_rate = 0.1, num_iters = 2000,
print_every = 50,
minibatch_size = NULL)
)
# Using full gradient
# profvis::profvis(
system.time(
dat_full <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
learn_rate = 0.1, num_iters = 2000,
print_every = 50,
minibatch_size = NULL)
)
# predictions from the minibatch model
beta_idx <- grepl("beta", names(dat_mini))
beta_fit_mini <- dat_mini[nrow(dat_mini), beta_idx]
y_pred_mini <- U %*% as.numeric(beta_fit_mini)
# predictions from the full model
beta_idx <- grepl("beta", names(dat_full))
beta_fit_full <- dat_full[nrow(dat_full), beta_idx]
y_pred_full <- U %*% as.numeric(beta_fit_full)
dat <- data.frame(x = locs[, 1], y = locs[, 2], z = drop(W %*% alpha), y_sim = y,
y_pred_mini = drop(y_pred_mini), y_pred_full = drop(y_pred_full))
p_mini <- ggplot(dat, aes(x, y, fill = y_pred_mini)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Minibatch fit")
p_full <- ggplot(dat, aes(x, y, fill = y_pred_full)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Full fit")
p_sim / p_mini / p_full
sd(y - y_pred_mini)
sd(y - y_pred_full)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
facet_wrap(~ name)
tU <- t(U)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0) +
facet_wrap(~ name)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red") +
facet_wrap(~ name)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red", lwd=2) +
facet_wrap(~ name)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red", lwd=2) +
facet_wrap(name ~)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red", lwd=2) +
facet_wrap(~ name, nrow=2)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red", lwd=2) +
facet_wrap(~ name, nrow=2) +
coord_cartesian()
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red", lwd=2) +
facet_wrap(~ name, nrow=2) +
coord_fixed()
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red", lwd=2) +
facet_wrap(~ name, nrow=2)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red", lwd=1.5) +
facet_wrap(~ name, nrow=2)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red") +
facet_wrap(~ name, nrow=2)
# profvis::profvis(
system.time(
dat_mini <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))*0.1,
threshold = 0.000001,
learn_rate = 0.1, num_iters = 2000,
print_every = 50,
minibatch_size = 2^8)
)
# profvis::profvis(
system.time(
dat_mini <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))*0.1,
threshold = 0.000001,
learn_rate = 0.1, num_iters = 2000,
print_every = 50,
minibatch_size = 2^8)
)
# Using full gradient
# profvis::profvis(
system.time(
dat_full <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))* 0.1,
threshold = 0.000001,
learn_rate = 0.1, num_iters = 2000,
print_every = 50,
minibatch_size = NULL)
)
# predictions from the minibatch model
beta_idx <- grepl("beta", names(dat_mini))
beta_fit_mini <- dat_mini[nrow(dat_mini), beta_idx]
y_pred_mini <- U %*% as.numeric(beta_fit_mini)
# predictions from the full model
beta_idx <- grepl("beta", names(dat_full))
beta_fit_full <- dat_full[nrow(dat_full), beta_idx]
y_pred_full <- U %*% as.numeric(beta_fit_full)
dat <- data.frame(x = locs[, 1], y = locs[, 2], z = drop(W %*% alpha), y_sim = y,
y_pred_mini = drop(y_pred_mini), y_pred_full = drop(y_pred_full))
p_mini <- ggplot(dat, aes(x, y, fill = y_pred_mini)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Minibatch fit")
p_full <- ggplot(dat, aes(x, y, fill = y_pred_full)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Full fit")
p_sim / p_mini / p_full
sd(y - y_pred_mini)
sd(y - y_pred_full)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red") +
facet_wrap(~ name, nrow=2)
tU <- t(U)
tUU <- t(U) %*% U
theta_test <- rnorm(ncol(U))
bm <- microbenchmark::microbenchmark(
tUU %*% theta_test,
tU %*% (U %*% theta_test), times = 10
)
MRA <- mra_wendland_2d(locs, M = M, n_coarse = n_coarse, use_spam = FALSE)
Q_alpha_tau2 <- make_Q_alpha_tau2(Q_alpha, tau2, use_spam = FALSE)
Q_alpha      <- make_Q_alpha_2d(sqrt(n_dims), rep(0.999, length(n_dims)), prec_model = "CAR", use_spam = FALSE)
tau2         <- 10 * 2^(2 * (1:M - 1))
Q_alpha_tau2 <- make_Q_alpha_tau2(Q_alpha, tau2, use_spam = FALSE)
str(Q_alpha_tau2)
## initialize the random effect
## set up a linear constraint so that each resolution sums to one
A_constraint <- sapply(1:M, function(i){
tmp = rep(0, sum(n_dims))
tmp[dims_idx == i] <- 1
return(tmp)
})
a_constraint <- rep(0, M)
alpha   <- as.vector(rmvnorm.prec.const(n = 1, mu = rep(0, sum(n_dims)), Q = Q_alpha_tau2, A = t(A_constraint), a = a_constraint))
str(alpha)
sigma2 <- 0.25
epsilon <- rnorm(N, 0, sqrt(sigma2))
y <- as.numeric(X %*% beta + W %*% alpha + sigma2)
dat <- data.frame(x=locs[, 1], y = locs[, 2], z = y)
p_sim <- ggplot(dat, aes(x, y, fill = z)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Simulated")
p_sim
y <- as.numeric(W %*% alpha + sigma2)
dat <- data.frame(x=locs[, 1], y = locs[, 2], z = y)
p_sim <- ggplot(dat, aes(x, y, fill = z)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Simulated")
p_sim
U <- cbind(X, W)
# profvis::profvis(
system.time(
dat_mini <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))*0.1,
threshold = 0.000001,
learn_rate = 0.1, num_iters = 2000,
print_every = 50,
minibatch_size = 2^8)
)
# Using full gradient
# profvis::profvis(
system.time(
dat_full <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))* 0.1,
threshold = 0.000001,
learn_rate = 0.1, num_iters = 2000,
print_every = 50,
minibatch_size = NULL)
)
# predictions from the minibatch model
beta_idx <- grepl("beta", names(dat_mini))
beta_fit_mini <- dat_mini[nrow(dat_mini), beta_idx]
y_pred_mini <- U %*% as.numeric(beta_fit_mini)
# predictions from the full model
beta_idx <- grepl("beta", names(dat_full))
beta_fit_full <- dat_full[nrow(dat_full), beta_idx]
y_pred_full <- U %*% as.numeric(beta_fit_full)
dat <- data.frame(x = locs[, 1], y = locs[, 2], z = drop(W %*% alpha), y_sim = y,
y_pred_mini = drop(y_pred_mini), y_pred_full = drop(y_pred_full))
p_mini <- ggplot(dat, aes(x, y, fill = y_pred_mini)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Minibatch fit")
p_full <- ggplot(dat, aes(x, y, fill = y_pred_full)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Full fit")
p_sim / p_mini / p_full
sd(y - y_pred_mini)
sd(y - y_pred_full)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red") +
facet_wrap(~ name, nrow=2)
N
N <- 400^2
## setup the spatial process
locs <- as.matrix(
expand.grid(
seq(0, 1, length.out = sqrt(N)),
seq(0, 1, length.out = sqrt(N))
)
)
## fixed effects include intercept, elevation, and latitude
X <- cbind(1, rnorm(N), locs[, 2])
# X <- cbind(1, as.vector(mvnfast::rmvn(1, rep(0, N), 3 * exp(-D / 20))), locs[, 2])
p <- ncol(X)
beta <- rnorm(ncol(X))
## MRA spatio-temporal random effect
M <- 3
n_coarse <- 20
MRA <- mra_wendland_2d(locs, M = M, n_coarse = n_coarse, use_spam = TRUE)
# MRA    <- mra_wendland_2d(locs, M = M, n_max_fine_grid = 2^8, use_spam = TRUE)
W <- as.dgCMatrix.spam(MRA$W) # using the Matrix package is faster here because we don't need the Choleksy
# W <- MRA$W
n_dims <- MRA$n_dims
dims_idx <- MRA$dims_idx
Q_alpha      <- make_Q_alpha_2d(sqrt(n_dims), rep(0.999, length(n_dims)), prec_model = "CAR", use_spam = FALSE)
tau2         <- 10 * 2^(2 * (1:M - 1))
Q_alpha_tau2 <- make_Q_alpha_tau2(Q_alpha, tau2, use_spam = FALSE)
## initialize the random effect
## set up a linear constraint so that each resolution sums to one
A_constraint <- sapply(1:M, function(i){
tmp = rep(0, sum(n_dims))
tmp[dims_idx == i] <- 1
return(tmp)
})
a_constraint <- rep(0, M)
alpha   <- as.vector(rmvnorm.prec.const(n = 1, mu = rep(0, sum(n_dims)), Q = Q_alpha_tau2, A = t(A_constraint), a = a_constraint))
sigma2 <- 0.25
epsilon <- rnorm(N, 0, sqrt(sigma2))
y <- as.numeric(W %*% alpha + sigma2)
dat <- data.frame(x=locs[, 1], y = locs[, 2], z = y)
p_sim <- ggplot(dat, aes(x, y, fill = z)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Simulated")
p_sim
U <- cbind(X, W)
# profvis::profvis(
system.time(
dat_mini <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))*0.1,
threshold = 0.000001,
learn_rate = 0.1, num_iters = 2000,
print_every = 50,
minibatch_size = 2^8)
)
# Using full gradient
# profvis::profvis(
system.time(
dat_full <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))* 0.1,
threshold = 0.000001,
learn_rate = 0.1, num_iters = 2000,
print_every = 50,
minibatch_size = NULL)
)
layout(matrix(1:2), 1, 2)
plot(dat_mini$loss)
plot(dat_full$loss)
layout(matrix(1:2, 1, 2))
layout(matrix(1:2, 1, 2))
dev.off()
layout(matrix(1:2, 1, 2))
plot(dat_mini$loss)
plot(dat_full$loss)
layout(matrix(1:2, 2, 1))
plot(dat_mini$loss)
plot(dat_full$loss)
plot(dat_mini$loss, type='l')
plot(dat_full$loss, type='l')
# Using full gradient -- This algorithm gets more competitive with sample size
# profvis::profvis(
system.time(
dat_full <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))* 0.1,
threshold = 0.000001,
learn_rate = 0.01, num_iters = 2000,
print_every = 50,
minibatch_size = NULL)
)
# Using full gradient -- This algorithm gets more competitive with sample size
# profvis::profvis(
system.time(
dat_full <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))* 0.1,
threshold = 0.000001,
learn_rate = 1, num_iters = 2000,
print_every = 50,
minibatch_size = NULL)
)
message("loss = ", target_fun(y, U, c(beta, alpha)))
layout(matrix(1:2, 2, 1))
plot(dat_mini$loss, type='l')
plot(dat_full$loss, type='l')
# predictions from the minibatch model
beta_idx <- grepl("beta", names(dat_mini))
beta_fit_mini <- dat_mini[nrow(dat_mini), beta_idx]
y_pred_mini <- U %*% as.numeric(beta_fit_mini)
# predictions from the full model
beta_idx <- grepl("beta", names(dat_full))
beta_fit_full <- dat_full[nrow(dat_full), beta_idx]
y_pred_full <- U %*% as.numeric(beta_fit_full)
dat <- data.frame(x = locs[, 1], y = locs[, 2], z = drop(W %*% alpha), y_sim = y,
y_pred_mini = drop(y_pred_mini), y_pred_full = drop(y_pred_full))
p_mini <- ggplot(dat, aes(x, y, fill = y_pred_mini)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Minibatch fit")
p_full <- ggplot(dat, aes(x, y, fill = y_pred_full)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Full fit")
p_sim / p_mini / p_full
sd(y - y_pred_mini)
sd(y - y_pred_full)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red") +
facet_wrap(~ name, nrow=2)
tU <- t(U)
tUU <- t(U) %*% U
theta_test <- rnorm(ncol(U))
str(W)
ncol(W)
## MRA spatio-temporal random effect
M <- 3
n_coarse <- 40
MRA <- mra_wendland_2d(locs, M = M, n_coarse = n_coarse, use_spam = TRUE)
# MRA    <- mra_wendland_2d(locs, M = M, n_max_fine_grid = 2^8, use_spam = TRUE)
W <- as.dgCMatrix.spam(MRA$W) # using the Matrix package is faster here because we don't need the Choleksy
ncol(W)
# MRA    <- mra_wendland_2d(locs, M = M, n_max_fine_grid = 2^8, use_spam = TRUE)
W <- as.dgCMatrix.spam(MRA$W) # using the Matrix package is faster here because we don't need the Choleksy
# W <- MRA$W
n_dims <- MRA$n_dims
dims_idx <- MRA$dims_idx
Q_alpha      <- make_Q_alpha_2d(sqrt(n_dims), rep(0.999, length(n_dims)), prec_model = "CAR", use_spam = FALSE)
tau2         <- 10 * 2^(2 * (1:M - 1))
Q_alpha_tau2 <- make_Q_alpha_tau2(Q_alpha, tau2, use_spam = FALSE)
## initialize the random effect
## set up a linear constraint so that each resolution sums to one
A_constraint <- sapply(1:M, function(i){
tmp = rep(0, sum(n_dims))
tmp[dims_idx == i] <- 1
return(tmp)
})
a_constraint <- rep(0, M)
alpha   <- as.vector(rmvnorm.prec.const(n = 1, mu = rep(0, sum(n_dims)), Q = Q_alpha_tau2, A = t(A_constraint), a = a_constraint))
sigma2 <- 0.25
epsilon <- rnorm(N, 0, sqrt(sigma2))
y <- as.numeric(W %*% alpha + sigma2)
dat <- data.frame(x=locs[, 1], y = locs[, 2], z = y)
p_sim <- ggplot(dat, aes(x, y, fill = z)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Simulated")
p_sim
N
U <- cbind(X, W)
message("loss = ", target_fun(y, U, c(beta, alpha)))
# profvis::profvis(
system.time(
dat_mini <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))*0.1,
threshold = 0.000001,
learn_rate = 1, num_iters = 2000,
print_every = 50,
minibatch_size = 2^8)
)
message("loss = ", target_fun(y, U, c(rep(0, length(beta)), alpha)))
# profvis::profvis(
system.time(
dat_mini <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))*0.1,
threshold = 0.000001,
learn_rate = 1, num_iters = 2000,
print_every = 50,
minibatch_size = 2^8)
)
# profvis::profvis(
system.time(
dat_mini <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))*0.1,
threshold = 0.000001,
learn_rate = 1, num_iters = 2000,
print_every = 50,
minibatch_size = 2^8)
)
# Using full gradient -- This algorithm gets more competitive with sample size
# profvis::profvis(
system.time(
dat_full <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U))* 0.1,
threshold = 0.000001,
learn_rate = 1, num_iters = 2000,
print_every = 50,
minibatch_size = NULL)
)
layout(matrix(1:2, 2, 1))
plot(dat_mini$loss, type='l')
plot(dat_full$loss, type='l')
# predictions from the minibatch model
beta_idx <- grepl("beta", names(dat_mini))
beta_fit_mini <- dat_mini[nrow(dat_mini), beta_idx]
y_pred_mini <- U %*% as.numeric(beta_fit_mini)
lines(dat_full$loss, , col='red', type='l')
plot(dat_mini$loss, type='l')
lines(dat_full$loss, , col='red', type='l')
dev.off()
plot(dat_mini$loss, type='l')
lines(dat_full$loss, , col='red', type='l')
# predictions from the minibatch model
beta_idx <- grepl("beta", names(dat_mini))
beta_fit_mini <- dat_mini[nrow(dat_mini), beta_idx]
y_pred_mini <- U %*% as.numeric(beta_fit_mini)
# predictions from the full model
beta_idx <- grepl("beta", names(dat_full))
beta_fit_full <- dat_full[nrow(dat_full), beta_idx]
y_pred_full <- U %*% as.numeric(beta_fit_full)
dat <- data.frame(x = locs[, 1], y = locs[, 2], z = drop(W %*% alpha), y_sim = y,
y_pred_mini = drop(y_pred_mini), y_pred_full = drop(y_pred_full))
p_mini <- ggplot(dat, aes(x, y, fill = y_pred_mini)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Minibatch fit")
p_full <- ggplot(dat, aes(x, y, fill = y_pred_full)) +
geom_raster() +
scale_fill_viridis_c() +
ggtitle("Full fit")
p_sim / p_mini / p_full
sd(y - y_pred_mini)
sd(y - y_pred_full)
dat %>%
pivot_longer(cols= y_pred_mini:y_pred_full) %>%
ggplot(aes(x = y_sim, y=value)) +
geom_hex() +
geom_abline(slope=1, intercept=0, color="red") +
facet_wrap(~ name, nrow=2)
tU <- t(U)
tUU <- t(U) %*% U
theta_test <- rnorm(ncol(U))
