geom_abline(color = 'red')
# Do this with BayesMRA
library(BayesMRA)
# TODO
# - add in a decaying learning rate
# - add in penalty term Q_alpha_tau2 with fixed penalty tau2
#     - add in adaptive penalty term tau2?
# - add in sum-to-zero constraint
#
# Do this with BayesMRA
library(tidyverse)
library(patchwork)
library(sgMRA)
library(BayesMRA)
# TODO
# - add in a decaying learning rate
# - add in penalty term Q_alpha_tau2 with fixed penalty tau2
#     - add in adaptive penalty term tau2?
# - add in sum-to-zero constraint
#
# Do this with BayesMRA
library(tidyverse)
library(patchwork)
library(sgMRA)
library(BayesMRA)
library(spam)
library(Matrix)
set.seed(11)
N <- 100^2
## setup the spatial process
locs <- as.matrix(
expand.grid(
seq(0, 1, length.out = sqrt(N)),
seq(0, 1, length.out = sqrt(N))
)
)
# D <- fields::rdist(locs)
## fixed effects include intercept, elevation, and latitude
X <- cbind(1, rnorm(N), locs[, 2])
# X <- cbind(1, as.vector(mvnfast::rmvn(1, rep(0, N), 3 * exp(-D / 20))), locs[, 2])
p <- ncol(X)
beta <- rnorm(ncol(X))
## MRA spatio-temporal random effect
M <- 3
n_coarse <- 30
MRA    <- mra_wendland_2d(locs, M = M, n_coarse = n_coarse, use_spam = TRUE)
# MRA    <- mra_wendland_2d(locs, M = M, n_max_fine_grid = 2^8, use_spam = TRUE)
W <- as.dgRMatrix.spam(MRA$W) # using the Matrix package is faster here because we don't need the Choleksy
# W <- MRA$W
n_dims <- MRA$n_dims
dims_idx <- MRA$dims_idx
Q_alpha      <- make_Q_alpha_2d(sqrt(n_dims), rep(0.999, length(n_dims)), prec_model = "CAR")
tau2         <- 10 * 2^(2 * (1:M - 1))
Q_alpha_tau2 <- make_Q_alpha_tau2(Q_alpha, tau2)
## initialize the random effect
## set up a linear constraint so that each resolution sums to one
A_constraint <- sapply(1:M, function(i){
tmp = rep(0, sum(n_dims))
tmp[dims_idx == i] <- 1
return(tmp)
})
a_constraint <- rep(0, M)
alpha   <- as.vector(rmvnorm.prec.const(n = 1, mu = rep(0, sum(n_dims)), Q = Q_alpha_tau2, A = t(A_constraint), a = a_constraint))
sigma2 <- runif(1, 0.25, 0.5)
y <- as.numeric(X %*% beta + W %*% alpha + rnorm(N, 0, sqrt(sigma2)))
U <- cbind(X, W)
# profvis::profvis(
system.time(
dat <- regression_gradient_descent(target_fun, gradient_fun, c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
alpha = 0.001, num_iters = 500, print_every = 10,
minibatch_size = 2^6)
)
# profvis::profvis(
system.time(
dat <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
alpha = 0.001, num_iters = 500, print_every = 10,
minibatch_size = 2^6)
)
str(X)
str(W)
cbind(X, W)
library(sgMRA)
# TODO
# - add in a decaying learning rate
# - add in penalty term Q_alpha_tau2 with fixed penalty tau2
#     - add in adaptive penalty term tau2?
# - add in sum-to-zero constraint
#
# Do this with BayesMRA
library(tidyverse)
library(patchwork)
library(sgMRA)
library(BayesMRA)
library(spam)
library(Matrix)
set.seed(11)
N <- 100^2
## setup the spatial process
locs <- as.matrix(
expand.grid(
seq(0, 1, length.out = sqrt(N)),
seq(0, 1, length.out = sqrt(N))
)
)
# D <- fields::rdist(locs)
## fixed effects include intercept, elevation, and latitude
X <- cbind(1, rnorm(N), locs[, 2])
# X <- cbind(1, as.vector(mvnfast::rmvn(1, rep(0, N), 3 * exp(-D / 20))), locs[, 2])
p <- ncol(X)
beta <- rnorm(ncol(X))
## MRA spatio-temporal random effect
M <- 3
n_coarse <- 30
MRA    <- mra_wendland_2d(locs, M = M, n_coarse = n_coarse, use_spam = TRUE)
# MRA    <- mra_wendland_2d(locs, M = M, n_max_fine_grid = 2^8, use_spam = TRUE)
W <- as.dgRMatrix.spam(MRA$W) # using the Matrix package is faster here because we don't need the Choleksy
# W <- MRA$W
n_dims <- MRA$n_dims
dims_idx <- MRA$dims_idx
Q_alpha      <- make_Q_alpha_2d(sqrt(n_dims), rep(0.999, length(n_dims)), prec_model = "CAR")
tau2         <- 10 * 2^(2 * (1:M - 1))
Q_alpha_tau2 <- make_Q_alpha_tau2(Q_alpha, tau2)
## initialize the random effect
## set up a linear constraint so that each resolution sums to one
A_constraint <- sapply(1:M, function(i){
tmp = rep(0, sum(n_dims))
tmp[dims_idx == i] <- 1
return(tmp)
})
a_constraint <- rep(0, M)
alpha   <- as.vector(rmvnorm.prec.const(n = 1, mu = rep(0, sum(n_dims)), Q = Q_alpha_tau2, A = t(A_constraint), a = a_constraint))
sigma2 <- runif(1, 0.25, 0.5)
y <- as.numeric(X %*% beta + W %*% alpha + rnorm(N, 0, sqrt(sigma2)))
# gradient descent function for MRA using minibatch ----
U <- cbind(X, W)
# profvis::profvis(
system.time(
dat <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
alpha = 0.001, num_iters = 500, print_every = 10,
minibatch_size = 2^6)
)
# Using full gradient
# profvis::profvis(
system.time(
dat <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
alpha = 0.1, num_iters = 500, print_every = 10,
minibatch_size = NULL)
)
# profvis::profvis(
system.time(
dat <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
alpha = 0.1, num_iters = 500, print_every = 10,
minibatch_size = 2^6)
)
tU <- t(U)
tUU <- t(U) %*% U
tUy <- t(U) %*% y
idx <- sample(1:length(y), 2^6)
theta_test <- rnorm(ncol(U))
gradient_fun(y[idx], U[idx, ], tUy[idx], tUU, theta_test)
y_idx <- y[idx]
tUy_idx <- tU[, idx] %*% y[idx]
gradient_fun(y[idx], tUy[idx], tUU, theta_test)
?gradient_fun
gradient_fun(y[idx], tUy[idx], tUU, theta_test)
str(y[idx])
str(tUy[idx])
str(tUy[idx,])
str(tUy)
str(theta_test)
theta_test <- rnorm(ncol(U))
gradient_fun(y[idx], tUy[idx], tUU, theta_test)
str(tUU %*% theta_test)
# Using full gradient
# profvis::profvis(
system.time(
dat <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
alpha = 0.1, num_iters = 500, print_every = 10,
minibatch_size = NULL)
)
tU <- t(U)
tUU <- t(U) %*% U
tUy <- t(U) %*% y
idx <- sample(1:length(y), 2^6)
theta_test <- rnorm(ncol(U))
gradient_fun(y[idx], tUy[idx], tUU, theta_test)
y_idx <- y[idx]
tUy_idx <- tU[, idx] %*% y_idx
y_idx <- y[idx]
tUy_idx <- tU[, idx] %*% y_idx
gradient_fun(y_idx, tUy_idx, tUU, theta_test)
bm <- microbenchmark::microbenchmark(
gradient_fun(y, tUy, tUU, theta_test),
gradient_fun(y[idx], tU[, idx] %*% y[idx], tUU, theta_test), times = 10)
autoplot(bm)
str(tY)
str(tU)
profvis::profvis(
# system.time(
dat <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
alpha = 0.001, num_iters = 500, print_every = 10,
minibatch_size = 2^6)
)
# TODO
# - add in a decaying learning rate
# - add in penalty term Q_alpha_tau2 with fixed penalty tau2
#     - add in adaptive penalty term tau2?
# - add in sum-to-zero constraint
#
# Do this with BayesMRA
library(tidyverse)
library(patchwork)
library(sgMRA)
library(BayesMRA)
library(spam)
library(Matrix)
set.seed(11)
N <- 400^2
## setup the spatial process
locs <- as.matrix(
expand.grid(
seq(0, 1, length.out = sqrt(N)),
seq(0, 1, length.out = sqrt(N))
)
)
# D <- fields::rdist(locs)
## fixed effects include intercept, elevation, and latitude
X <- cbind(1, rnorm(N), locs[, 2])
# X <- cbind(1, as.vector(mvnfast::rmvn(1, rep(0, N), 3 * exp(-D / 20))), locs[, 2])
p <- ncol(X)
beta <- rnorm(ncol(X))
## MRA spatio-temporal random effect
M <- 4
n_coarse <- 30
MRA    <- mra_wendland_2d(locs, M = M, n_coarse = n_coarse, use_spam = TRUE)
# MRA    <- mra_wendland_2d(locs, M = M, n_max_fine_grid = 2^8, use_spam = TRUE)
W <- as.dgRMatrix.spam(MRA$W) # using the Matrix package is faster here because we don't need the Choleksy
# W <- MRA$W
n_dims <- MRA$n_dims
dims_idx <- MRA$dims_idx
Q_alpha      <- make_Q_alpha_2d(sqrt(n_dims), rep(0.999, length(n_dims)), prec_model = "CAR")
tau2         <- 10 * 2^(2 * (1:M - 1))
Q_alpha_tau2 <- make_Q_alpha_tau2(Q_alpha, tau2)
## initialize the random effect
## set up a linear constraint so that each resolution sums to one
A_constraint <- sapply(1:M, function(i){
tmp = rep(0, sum(n_dims))
tmp[dims_idx == i] <- 1
return(tmp)
})
a_constraint <- rep(0, M)
alpha   <- as.vector(rmvnorm.prec.const(n = 1, mu = rep(0, sum(n_dims)), Q = Q_alpha_tau2, A = t(A_constraint), a = a_constraint))
sigma2 <- runif(1, 0.25, 0.5)
y <- as.numeric(X %*% beta + W %*% alpha + rnorm(N, 0, sqrt(sigma2)))
# gradient descent function for MRA using minibatch ----
U <- cbind(X, W)
profvis::profvis(
# system.time(
dat <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
alpha = 0.001, num_iters = 500, print_every = 10,
minibatch_size = 2^6)
)
profvis::profvis(
# system.time(
dat <- regression_gradient_descent(c(y),
X, W, inits = rnorm(ncol(U)),
threshold = 0.000001,
alpha = 0.0001, num_iters = 500, print_every = 10,
minibatch_size = 2^6)
)
tU <- t(U)
tUU <- t(U) %*% U
theta_test <- rnorm(ncol(U))
bm <- microbenchmark::microbenchmark(
tUU %*% theta_test,
tU %*% (U %*% theta_test), times = 10
)
autoplot(bm)
# MRA    <- mra_wendland_2d(locs, M = M, n_max_fine_grid = 2^8, use_spam = TRUE)
W <- as.dgCMatrix.spam(MRA$W) # using the Matrix package is faster here because we don't need the Choleksy
str(W)
library(reticulate)
)
py_config()
nn
n
xaringan:::inf_mr()
# TODO
# - work on fitting these using stochastic gradient descent or elliptical slice sampling
# Deep BayesMRA
library(spam)
library(Matrix)
library(igraph)
library(tidyverse)
library(BayesMRA)
library(patchwork)
set.seed(44)
N <- 100^2
locs <- expand_grid(x=seq(0, 1, length.out=sqrt(N)),
y=seq(0, 1, length.out=sqrt(N)))
# construct the first layer
MRA1 <- mra_wendland_2d(as.matrix(locs), M=1, n_coarse_grid = 20)
W1 <- MRA1$W
Q1 <- make_Q_alpha_2d(sqrt(MRA1$n_dims), phi=0.9)
class(Q1) <- "spam"
alpha_x1 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q1)), Q = Q1))
alpha_y1 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q1)), Q = Q1))
# construct the second layer
# MRA2 <- mra_wendland_2d(cbind(W1 %*% alpha_x1, W1 %*% alpha_y1), M=1, n_coarse_grid = 20)
# # MRA2 <- mra_wendland_2d_pred(cbind(W1 %*% alpha_x1, W1 %*% alpha_y1), MRA1)
# W2 <- MRA2$W
# Q2 <- make_Q_alpha_2d(sqrt(MRA2$n_dims), phi=0.9)
# class(Q2) <- "spam"
# alpha_x2 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q2)), Q = Q2))
# alpha_y2 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q2)), Q = Q2))
# construct the final layer
MRA <- mra_wendland_2d(cbind(W1 %*% alpha_x1, W1 %*% alpha_y1), M=1, n_coarse_grid = 20)
# MRA <- mra_wendland_2d_pred(cbind(W2 %*% alpha_x2, W2 %*% alpha_y2), MRA1)
W <- MRA$W
Q <- make_Q_alpha_2d(sqrt(MRA$n_dims), phi=0.9)
class(Q) <- "spam"
alpha <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q)), Q = Q))
z <- W %*% alpha
sigma <- 0.5
epsilon <- rnorm(N, 0, sigma)
y_obs <- z + epsilon
dat <- data.frame(x = locs$x, y = locs$y, z = z, y_obs = y_obs)
p1 <- ggplot(dat, aes(x = x, y = y, fill = z)) +
geom_raster() +
scale_fill_viridis_c()
p2 <- ggplot(dat, aes(x = x, y = y, fill = y_obs)) +
geom_raster() +
scale_fill_viridis_c()
p1 + p2
# TODO
# - work on fitting these using stochastic gradient descent or elliptical slice sampling
# Deep BayesMRA
library(spam)
library(Matrix)
library(igraph)
library(tidyverse)
library(BayesMRA)
library(patchwork)
set.seed(44)
N <- 100^2
locs <- expand_grid(x=seq(0, 1, length.out=sqrt(N)),
y=seq(0, 1, length.out=sqrt(N)))
# construct the first layer
MRA1 <- mra_wendland_2d(as.matrix(locs), M=1, n_coarse_grid = 20)
W1 <- MRA1$W
Q1 <- make_Q_alpha_2d(sqrt(MRA1$n_dims), phi=0.9)
class(Q1) <- "spam"
alpha_x1 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q1)), Q = Q1))
alpha_y1 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q1)), Q = Q1))
# construct the second layer
MRA2 <- mra_wendland_2d(cbind(W1 %*% alpha_x1, W1 %*% alpha_y1), M=1, n_coarse_grid = 20)
# MRA2 <- mra_wendland_2d_pred(cbind(W1 %*% alpha_x1, W1 %*% alpha_y1), MRA1)
W2 <- MRA2$W
Q2 <- make_Q_alpha_2d(sqrt(MRA2$n_dims), phi=0.9)
class(Q2) <- "spam"
alpha_x2 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q2)), Q = Q2))
alpha_y2 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q2)), Q = Q2))
# construct the final layer
MRA <- mra_wendland_2d(cbind(W2 %*% alpha_x2, W2 %*% alpha_y2), M=1, n_coarse_grid = 20)
# MRA <- mra_wendland_2d_pred(cbind(W2 %*% alpha_x2, W2 %*% alpha_y2), MRA1)
W <- MRA$W
Q <- make_Q_alpha_2d(sqrt(MRA$n_dims), phi=0.9)
class(Q) <- "spam"
alpha <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q)), Q = Q))
z <- W %*% alpha
sigma <- 0.5
epsilon <- rnorm(N, 0, sigma)
y_obs <- z + epsilon
dat <- data.frame(x = locs$x, y = locs$y, z = z, y_obs = y_obs)
p1 <- ggplot(dat, aes(x = x, y = y, fill = z)) +
geom_raster() +
scale_fill_viridis_c()
p2 <- ggplot(dat, aes(x = x, y = y, fill = y_obs)) +
geom_raster() +
scale_fill_viridis_c()
p1 + p2
z <- W1 %*% alpha
sigma <- 0.5
epsilon <- rnorm(N, 0, sigma)
y_obs <- z + epsilon
dat <- data.frame(x = locs$x, y = locs$y, z = z, y_obs = y_obs)
p1 <- ggplot(dat, aes(x = x, y = y, fill = z)) +
geom_raster() +
scale_fill_viridis_c()
p2 <- ggplot(dat, aes(x = x, y = y, fill = y_obs)) +
geom_raster() +
scale_fill_viridis_c()
p1 + p2
z <- W2 %*% alpha
sigma <- 0.5
epsilon <- rnorm(N, 0, sigma)
y_obs <- z + epsilon
dat <- data.frame(x = locs$x, y = locs$y, z = z, y_obs = y_obs)
p1 <- ggplot(dat, aes(x = x, y = y, fill = z)) +
geom_raster() +
scale_fill_viridis_c()
p2 <- ggplot(dat, aes(x = x, y = y, fill = y_obs)) +
geom_raster() +
scale_fill_viridis_c()
p1 + p2
z <- W %*% alpha
sigma <- 0.5
epsilon <- rnorm(N, 0, sigma)
y_obs <- z + epsilon
dat <- data.frame(x = locs$x, y = locs$y, z = z, y_obs = y_obs)
p1 <- ggplot(dat, aes(x = x, y = y, fill = z)) +
geom_raster() +
scale_fill_viridis_c()
p2 <- ggplot(dat, aes(x = x, y = y, fill = y_obs)) +
geom_raster() +
scale_fill_viridis_c()
p1 + p2
dat <- data.frame(x = locs$x, y = locs$y, layer = rep(1:3, each=N),
z = c(W1 %*% alpha, W2 %*% alpha, W3 %*% alpha))
dat <- data.frame(x = locs$x, y = locs$y, layer = rep(1:3, each=N),
z = c(W1 %*% alpha, W2 %*% alpha, W %*% alpha))
ggplot(dat, aes(x, y, fill=z)) +
geom_raster() +
scale_fill_viridis_c() +
facet_wrap(~ layer, ncol=3)
dat <- data.frame(x = locs$x, y = locs$y, layer = rep(1:3, each=N),
z = c(W1 %*% alpha_x1, W2 %*% alpha, W %*% alpha))
ggplot(dat, aes(x, y, fill=z)) +
geom_raster() +
scale_fill_viridis_c() +
facet_wrap(~ layer, ncol=3)
dat <- data.frame(x = locs$x, y = locs$y, layer = rep(c(1, 1, 2, 2, 3), each=N),
group = rep(c("x", "y", "x", "y", "z"), each = N),
z = c(W1 %*% alpha_x1, W1 %*% alpha_y1, W2 %*% alpha, W %*% alpha))
dat <- data.frame(x = locs$x, y = locs$y, layer = rep(c(1, 1, 2, 2, 3), each=N),
group = rep(c("x", "y", "x", "y", "z"), each = N),
z = c(W1 %*% alpha_x1, W1 %*% alpha_y1,
W2 %*% alpha_x2, W2 %*% alpha_y2,
W %*% alpha))
ggplot(dat, aes(x, y, fill=z)) +
geom_raster() +
scale_fill_viridis_c() +
facet_wrap(group~ layer, ncol=3)
ggplot(dat, aes(x, y, fill=z)) +
geom_raster() +
scale_fill_viridis_c() +
facet_grid(group~ layer, ncol=3)
ggplot(dat, aes(x, y, fill=z)) +
geom_raster() +
scale_fill_viridis_c() +
facet_grid(layer~ group)
p1
# TODO
# - work on fitting these using stochastic gradient descent or elliptical slice sampling
# Deep BayesMRA
library(spam)
library(Matrix)
library(igraph)
library(tidyverse)
library(BayesMRA)
library(patchwork)
set.seed(44)
N <- 400^2
locs <- expand_grid(x=seq(0, 1, length.out=sqrt(N)),
y=seq(0, 1, length.out=sqrt(N)))
# construct the first layer
MRA1 <- mra_wendland_2d(as.matrix(locs), M=1, n_coarse_grid = 20)
W1 <- MRA1$W
Q1 <- make_Q_alpha_2d(sqrt(MRA1$n_dims), phi=0.9)
class(Q1) <- "spam"
alpha_x1 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q1)), Q = Q1))
alpha_y1 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q1)), Q = Q1))
# construct the second layer
MRA2 <- mra_wendland_2d(cbind(W1 %*% alpha_x1, W1 %*% alpha_y1), M=1, n_coarse_grid = 20)
# MRA2 <- mra_wendland_2d_pred(cbind(W1 %*% alpha_x1, W1 %*% alpha_y1), MRA1)
W2 <- MRA2$W
Q2 <- make_Q_alpha_2d(sqrt(MRA2$n_dims), phi=0.9)
class(Q2) <- "spam"
alpha_x2 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q2)), Q = Q2))
alpha_y2 <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q2)), Q = Q2))
# construct the final layer
MRA <- mra_wendland_2d(cbind(W2 %*% alpha_x2, W2 %*% alpha_y2), M=1, n_coarse_grid = 20)
# MRA <- mra_wendland_2d_pred(cbind(W2 %*% alpha_x2, W2 %*% alpha_y2), MRA1)
W <- MRA$W
Q <- make_Q_alpha_2d(sqrt(MRA$n_dims), phi=0.9)
class(Q) <- "spam"
alpha <- drop(rmvnorm.prec(1, mu = rep(0, nrow(Q)), Q = Q))
z <- W %*% alpha
sigma <- 0.5
epsilon <- rnorm(N, 0, sigma)
y_obs <- z + epsilon
dat <- data.frame(x = locs$x, y = locs$y, z = z, y_obs = y_obs)
p1 <- ggplot(dat, aes(x = x, y = y, fill = z)) +
geom_raster() +
scale_fill_viridis_c()
p1
