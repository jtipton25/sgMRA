#     scale_fill_viridis_c() +
#     facet_grid(layer ~ group) +
#     ggtitle("simulated layers")
#
# p_layers_sim
message("Simulated loss:", 1 / (2 * N) * sum((y - z)^2))
# Fit the model using sgd ----
# source("~/sgMRA/scripts/fit-deep-MRA-sgd.R")
# source("~/sgMRA/R/adam.R")
n_iter = 500
n_iter=10
# add in Adam optimization schedule
profvis::profvis({
# system.time({
out <- fit_sgd(y = y, locs = locs, grid = grid,
alpha=NULL,
alpha_x1=NULL,
alpha_y1=NULL,
alpha_x2=NULL,
alpha_y2=NULL,
# alpha=alpha,
# alpha_x1=alpha_x1,
# alpha_y1=alpha_y1,
# alpha_x2=alpha_x2,
# alpha_y2=alpha_y2,
learn_rate = 0.1,
n_iter = n_iter,
n_message = 50,
penalized=TRUE,
plot_during_fit = TRUE)
})
library(sgMRA)
# system.time({
out <- fit_sgd(y = y, locs = locs, grid = grid,
alpha=NULL,
alpha_x1=NULL,
alpha_y1=NULL,
alpha_x2=NULL,
alpha_y2=NULL,
# alpha=alpha,
# alpha_x1=alpha_x1,
# alpha_y1=alpha_y1,
# alpha_x2=alpha_x2,
# alpha_y2=alpha_y2,
learn_rate = 0.1,
n_iter = n_iter,
n_message = 50,
penalized=TRUE,
plot_during_fit = TRUE)
# add in Adam optimization schedule
# profvis::profvis({
# system.time({
out <- fit_sgd(y = y, locs = locs, grid = grid,
alpha=NULL,
alpha_x1=NULL,
alpha_y1=NULL,
alpha_x2=NULL,
alpha_y2=NULL,
# alpha=alpha,
# alpha_x1=alpha_x1,
# alpha_y1=alpha_y1,
# alpha_x2=alpha_x2,
# alpha_y2=alpha_y2,
learn_rate = 0.1,
n_iter = n_iter,
n_message = 50,
penalized=TRUE,
plot_during_fit = TRUE)
# initialize the MRA parameters here
MRA2 <- eval_basis(locs, grid, use_spam=use_spam)
str(locs)
str(grid)
use_spam = FALSE
# initialize the MRA parameters here
MRA2 <- eval_basis(locs, grid, use_spam=use_spam)
str(locs)
str(grid)
# TODO
# - work on fitting these using stochastic gradient descent or elliptical slice sampling
# Deep BayesMRA
library(spam)
library(Matrix)
library(igraph)
library(tidyverse)
library(BayesMRA)
library(patchwork)
library(sgMRA)
source("~/sgMRA/R/eval_basis.R")
Rcpp::sourceCpp("~/sgMRA/src/dist_near_cpp.cpp")
source("~/sgMRA/R/dwendland_basis.R")
set.seed(44)
N <- 2^12
locs <- expand_grid(x=seq(0, 1, length.out=sqrt(N)),
y=seq(0, 1, length.out=sqrt(N)))
idx1 <- ((locs$x > 1/4) & (locs$x <= 3/4)) & ((locs$y > 1/4) & (locs$y <= 3/4))
idx2 <- ((locs$x > 1/2) & (locs$x <= 3/4)) & ((locs$y > 1/2) & (locs$y <= 3/4))
idx3 <- ((locs$x > 1/4) & (locs$x <= 1/2)) & ((locs$y > 1/4) & (locs$y <= 1/2))
z <- cos(2*pi*locs$x) * cos(2*pi*locs$y)
z[idx1] <- z[idx1] + sin(4*pi*locs$x[idx1]) * sin(4*pi*locs$y[idx1])
z[idx2] <- z[idx2] + sin(8*pi*locs$x[idx2]) * sin(8*pi*locs$y[idx2])
z[idx3] <- z[idx3] + sin(16*pi*locs$x[idx3]) * sin(16*pi*locs$y[idx3])
z <- 2*z
M <- 3
n_coarse_grid <- 45
grid <- make_grid(locs, M = M, n_coarse_grid = n_coarse_grid)
MRA <- eval_basis(locs, grid, use_spam = FALSE)
dim(MRA$W)
sigma <- 0.05
epsilon <- rnorm(N, 0, sigma)
y_obs <- z + epsilon
y <- y_obs
dat <- data.frame(x = locs$x, y = locs$y, z = z, y_obs = y_obs)
p1 <- ggplot(dat, aes(x = x, y = y, fill = z)) +
geom_raster() +
# scale_fill_distiller(palette = "RdYlBu")
scale_fill_viridis_c()
p2 <- ggplot(dat, aes(x = x, y = y, fill = y_obs)) +
geom_raster() +
# scale_fill_distiller(palette = "RdYlBu")
scale_fill_viridis_c()
p1 + p2
# dat <- data.frame(x = locs$x, y = locs$y,
#                   # layer = rep(c(1, 1, 2, 2, 3), each=N),
#                   layer = rep(c(1, 1, 2), each=N),
#                   group = rep(c("x", "y", "z"), each = N),
#                   # group = rep(c("x", "y", "x", "y", "z"), each = N),
#                   z = c(W1 %*% alpha_x1, W1 %*% alpha_y1,
#                         # W2 %*% alpha_x2, W2 %*% alpha_y2,
#                         W %*% alpha))
# p_layers_sim <- ggplot(dat, aes(x, y, fill=z)) +
#     geom_raster() +
#     scale_fill_viridis_c() +
#     facet_grid(layer ~ group) +
#     ggtitle("simulated layers")
#
# p_layers_sim
message("Simulated loss:", 1 / (2 * N) * sum((y - z)^2))
# Fit the model using sgd ----
# source("~/sgMRA/scripts/fit-deep-MRA-sgd.R")
# source("~/sgMRA/R/adam.R")
n_iter = 500
library(sgMRA)
# Deep BayesMRA
library(spam)
library(Matrix)
library(igraph)
library(tidyverse)
library(BayesMRA)
library(patchwork)
library(sgMRA)
source("~/sgMRA/R/eval_basis.R")
Rcpp::sourceCpp("~/sgMRA/src/dist_near_cpp.cpp")
source("~/sgMRA/R/dwendland_basis.R")
set.seed(44)
N <- 2^12
locs <- expand_grid(x=seq(0, 1, length.out=sqrt(N)),
y=seq(0, 1, length.out=sqrt(N)))
idx1 <- ((locs$x > 1/4) & (locs$x <= 3/4)) & ((locs$y > 1/4) & (locs$y <= 3/4))
idx2 <- ((locs$x > 1/2) & (locs$x <= 3/4)) & ((locs$y > 1/2) & (locs$y <= 3/4))
idx3 <- ((locs$x > 1/4) & (locs$x <= 1/2)) & ((locs$y > 1/4) & (locs$y <= 1/2))
z <- cos(2*pi*locs$x) * cos(2*pi*locs$y)
z[idx1] <- z[idx1] + sin(4*pi*locs$x[idx1]) * sin(4*pi*locs$y[idx1])
z[idx2] <- z[idx2] + sin(8*pi*locs$x[idx2]) * sin(8*pi*locs$y[idx2])
z[idx3] <- z[idx3] + sin(16*pi*locs$x[idx3]) * sin(16*pi*locs$y[idx3])
z <- 2*z
M <- 3
n_coarse_grid <- 45
grid <- make_grid(locs, M = M, n_coarse_grid = n_coarse_grid)
MRA <- eval_basis(locs, grid, use_spam = FALSE)
dim(MRA$W)
sigma <- 0.05
epsilon <- rnorm(N, 0, sigma)
y_obs <- z + epsilon
y <- y_obs
dat <- data.frame(x = locs$x, y = locs$y, z = z, y_obs = y_obs)
p1 <- ggplot(dat, aes(x = x, y = y, fill = z)) +
geom_raster() +
# scale_fill_distiller(palette = "RdYlBu")
scale_fill_viridis_c()
p2 <- ggplot(dat, aes(x = x, y = y, fill = y_obs)) +
geom_raster() +
# scale_fill_distiller(palette = "RdYlBu")
scale_fill_viridis_c()
p1 + p2
# dat <- data.frame(x = locs$x, y = locs$y,
#                   # layer = rep(c(1, 1, 2, 2, 3), each=N),
#                   layer = rep(c(1, 1, 2), each=N),
#                   group = rep(c("x", "y", "z"), each = N),
#                   # group = rep(c("x", "y", "x", "y", "z"), each = N),
#                   z = c(W1 %*% alpha_x1, W1 %*% alpha_y1,
#                         # W2 %*% alpha_x2, W2 %*% alpha_y2,
#                         W %*% alpha))
# p_layers_sim <- ggplot(dat, aes(x, y, fill=z)) +
#     geom_raster() +
#     scale_fill_viridis_c() +
#     facet_grid(layer ~ group) +
#     ggtitle("simulated layers")
#
# p_layers_sim
message("Simulated loss:", 1 / (2 * N) * sum((y - z)^2))
# Fit the model using sgd ----
# source("~/sgMRA/scripts/fit-deep-MRA-sgd.R")
# source("~/sgMRA/R/adam.R")
n_iter = 500
n_iter=10
# TODO
# - work on fitting these using stochastic gradient descent or elliptical slice sampling
# Deep BayesMRA
library(spam)
library(Matrix)
library(igraph)
library(tidyverse)
library(BayesMRA)
library(patchwork)
library(sgMRA)
# source("~/sgMRA/R/eval_basis.R")
# Rcpp::sourceCpp("~/sgMRA/src/dist_near_cpp.cpp")
# source("~/sgMRA/R/dwendland_basis.R")
set.seed(44)
N <- 2^12
locs <- expand_grid(x=seq(0, 1, length.out=sqrt(N)),
y=seq(0, 1, length.out=sqrt(N)))
idx1 <- ((locs$x > 1/4) & (locs$x <= 3/4)) & ((locs$y > 1/4) & (locs$y <= 3/4))
idx2 <- ((locs$x > 1/2) & (locs$x <= 3/4)) & ((locs$y > 1/2) & (locs$y <= 3/4))
idx3 <- ((locs$x > 1/4) & (locs$x <= 1/2)) & ((locs$y > 1/4) & (locs$y <= 1/2))
z <- cos(2*pi*locs$x) * cos(2*pi*locs$y)
z[idx1] <- z[idx1] + sin(4*pi*locs$x[idx1]) * sin(4*pi*locs$y[idx1])
z[idx2] <- z[idx2] + sin(8*pi*locs$x[idx2]) * sin(8*pi*locs$y[idx2])
z[idx3] <- z[idx3] + sin(16*pi*locs$x[idx3]) * sin(16*pi*locs$y[idx3])
z <- 2*z
M <- 3
n_coarse_grid <- 45
grid <- make_grid(locs, M = M, n_coarse_grid = n_coarse_grid)
MRA <- eval_basis(locs, grid, use_spam = FALSE)
dim(MRA$W)
sigma <- 0.05
epsilon <- rnorm(N, 0, sigma)
y_obs <- z + epsilon
y <- y_obs
dat <- data.frame(x = locs$x, y = locs$y, z = z, y_obs = y_obs)
p1 <- ggplot(dat, aes(x = x, y = y, fill = z)) +
geom_raster() +
# scale_fill_distiller(palette = "RdYlBu")
scale_fill_viridis_c()
p2 <- ggplot(dat, aes(x = x, y = y, fill = y_obs)) +
geom_raster() +
# scale_fill_distiller(palette = "RdYlBu")
scale_fill_viridis_c()
p1 + p2
# dat <- data.frame(x = locs$x, y = locs$y,
#                   # layer = rep(c(1, 1, 2, 2, 3), each=N),
#                   layer = rep(c(1, 1, 2), each=N),
#                   group = rep(c("x", "y", "z"), each = N),
#                   # group = rep(c("x", "y", "x", "y", "z"), each = N),
#                   z = c(W1 %*% alpha_x1, W1 %*% alpha_y1,
#                         # W2 %*% alpha_x2, W2 %*% alpha_y2,
#                         W %*% alpha))
# p_layers_sim <- ggplot(dat, aes(x, y, fill=z)) +
#     geom_raster() +
#     scale_fill_viridis_c() +
#     facet_grid(layer ~ group) +
#     ggtitle("simulated layers")
#
# p_layers_sim
message("Simulated loss:", 1 / (2 * N) * sum((y - z)^2))
# Fit the model using sgd ----
# sou
n_iter=10
# add in Adam optimization schedule
profvis::profvis({
# system.time({
out <- fit_sgd(y = y, locs = locs, grid = grid,
alpha=NULL,
alpha_x1=NULL,
alpha_y1=NULL,
alpha_x2=NULL,
alpha_y2=NULL,
# alpha=alpha,
# alpha_x1=alpha_x1,
# alpha_y1=alpha_y1,
# alpha_x2=alpha_x2,
# alpha_y2=alpha_y2,
learn_rate = 0.1,
n_iter = n_iter,
n_message = 50,
penalized=TRUE,
plot_during_fit = TRUE)
})
# add in Adam optimization schedule
# profvis::profvis({
# system.time({
out <- fit_sgd(y = y, locs = locs, grid = grid,
alpha=NULL,
alpha_x1=NULL,
alpha_y1=NULL,
alpha_x2=NULL,
alpha_y2=NULL,
# alpha=alpha,
# alpha_x1=alpha_x1,
# alpha_y1=alpha_y1,
# alpha_x2=alpha_x2,
# alpha_y2=alpha_y2,
learn_rate = 0.1,
n_iter = n_iter,
n_message = 50,
penalized=TRUE,
plot_during_fit = TRUE)
?make_Q
# initialize the MRA parameters here
MRA2 <- eval_basis(locs, grid, use_spam=use_spam)
use_spam = FALSE
# initialize the MRA parameters here
MRA2 <- eval_basis(locs, grid, use_spam=use_spam)
str(MRA2)
Q2 <- make_Q(sqrt(MRA2$n_dims), phi=rep(0.9, length(MRA2$n_dims)), use_spam=use_spam)
str(Q2)
if (!use_spam) {
CH2 <- Cholesky(Q2)
}
if (is.null(alpha_x2)) {
# alpha_x2 <- rnorm(ncol(MRA2$W), 0, 0.1)
# alpha_x2 <- rnorm(ncol(MRA2$W), 0, 1)
if (use_spam) {
alpha_x2 <- drop(rmvnorm.prec(1, rep(0, ncol(MRA2$W)), Q2)) * 0.1
} else {
alpha_x2 <- drop(sparseMVN::rmvn.sparse(1, rep(0, ncol(MRA2$W)), CH2, prec = TRUE)) * 0.1
}
}
alpha_x2 <- drop(sparseMVN::rmvn.sparse(1, rep(0, ncol(MRA2$W)), CH2, prec = TRUE)) * 0.1
alpha_y2 <- drop(sparseMVN::rmvn.sparse(1, rep(0, ncol(MRA2$W)), CH2, prec=TRUE)) * 0.1
MRA1 <- eval_basis(cbind(MRA2$W %*% alpha_x2, MRA2$W %*% alpha_y2), grid, use_spam=use_spam)
Q1 <- make_Q(sqrt(MRA1$n_dims), phi=rep(0.9, length(MRA1$n_dims)), use_spam=use_spam)
CH1 <- Cholesky(Q1)
alpha_x1 <- drop(sparseMVN::rmvn.sparse(1, rep(0, ncol(MRA1$W)), CH1, prec = TRUE)) * 0.1
alpha_y1 <- drop(sparseMVN::rmvn.sparse(1, rep(0, ncol(MRA1$W)), CH1, prec=TRUE)) * 0.1
alpha_x2 <- drop(sparseMVN::rmvn.sparse(1, rep(0, ncol(MRA2$W)), CH2, prec = TRUE)) * 0.1
MRA1 <- eval_basis(cbind(MRA2$W %*% alpha_x2, MRA2$W %*% alpha_y2), grid, use_spam=use_spam)
Q1 <- make_Q(sqrt(MRA1$n_dims), phi=rep(0.9, length(MRA1$n_dims)), use_spam=use_spam)
CH1 <- Cholesky(Q1)
alpha_x1 <- drop(sparseMVN::rmvn.sparse(1, rep(0, ncol(MRA1$W)), CH1, prec = TRUE)) * 0.1
alpha_y1 <- drop(sparseMVN::rmvn.sparse(1, rep(0, ncol(MRA1$W)), CH1, prec=TRUE)) * 0.1
MRA <- eval_basis(cbind(MRA1$W %*% alpha_x1, MRA1$W %*% alpha_y1), grid, use_spam=use_spam)
Q <- make_Q(sqrt(MRA$n_dims), phi=rep(0.9, length(MRA$n_dims)), use_spam=use_spam)
CH <- Cholesky(Q)
alpha <- drop(sparseMVN::rmvn.sparse(1, rep(0, ncol(MRA$W)), CH, prec=TRUE)) * 0.1
message("Initializing the model, the initialization loss is = ", 1 / (2 * N) * sum((y - MRA$W %*% alpha)^2))
# add in Adam optimization schedule
# profvis::profvis({
# system.time({
out <- fit_sgd(y = y, locs = locs, grid = grid,
alpha=NULL,
alpha_x1=NULL,
alpha_y1=NULL,
alpha_x2=NULL,
alpha_y2=NULL,
# alpha=alpha,
# alpha_x1=alpha_x1,
# alpha_y1=alpha_y1,
# alpha_x2=alpha_x2,
# alpha_y2=alpha_y2,
learn_rate = 0.1,
n_iter = n_iter,
n_message = 50,
penalized=TRUE,
plot_during_fit = TRUE)
library(sgMRA)
# TODO
# - work on fitting these using stochastic gradient descent or elliptical slice sampling
# Deep BayesMRA
library(spam)
library(Matrix)
library(igraph)
library(tidyverse)
library(BayesMRA)
library(patchwork)
library(sgMRA)
# source("~/sgMRA/R/eval_basis.R")
# Rcpp::sourceCpp("~/sgMRA/src/dist_near_cpp.cpp")
# source("~/sgMRA/R/dwendland_basis.R")
set.seed(44)
N <- 2^12
locs <- expand_grid(x=seq(0, 1, length.out=sqrt(N)),
y=seq(0, 1, length.out=sqrt(N)))
idx1 <- ((locs$x > 1/4) & (locs$x <= 3/4)) & ((locs$y > 1/4) & (locs$y <= 3/4))
idx2 <- ((locs$x > 1/2) & (locs$x <= 3/4)) & ((locs$y > 1/2) & (locs$y <= 3/4))
idx3 <- ((locs$x > 1/4) & (locs$x <= 1/2)) & ((locs$y > 1/4) & (locs$y <= 1/2))
z <- cos(2*pi*locs$x) * cos(2*pi*locs$y)
z[idx1] <- z[idx1] + sin(4*pi*locs$x[idx1]) * sin(4*pi*locs$y[idx1])
z[idx2] <- z[idx2] + sin(8*pi*locs$x[idx2]) * sin(8*pi*locs$y[idx2])
z[idx3] <- z[idx3] + sin(16*pi*locs$x[idx3]) * sin(16*pi*locs$y[idx3])
z <- 2*z
M <- 3
n_coarse_grid <- 45
grid <- make_grid(locs, M = M, n_coarse_grid = n_coarse_grid)
MRA <- eval_basis(locs, grid, use_spam = FALSE)
dim(MRA$W)
sigma <- 0.05
epsilon <- rnorm(N, 0, sigma)
y_obs <- z + epsilon
y <- y_obs
dat <- data.frame(x = locs$x, y = locs$y, z = z, y_obs = y_obs)
p1 <- ggplot(dat, aes(x = x, y = y, fill = z)) +
geom_raster() +
# scale_fill_distiller(palette = "RdYlBu")
scale_fill_viridis_c()
p2 <- ggplot(dat, aes(x = x, y = y, fill = y_obs)) +
geom_raster() +
# scale_fill_distiller(palette = "RdYlBu")
scale_fill_viridis_c()
p1 + p2
# dat <- data.frame(x = locs$x, y = locs$y,
#                   # layer = rep(c(1, 1, 2, 2, 3), each=N),
#                   layer = rep(c(1, 1, 2), each=N),
#                   group = rep(c("x", "y", "z"), each = N),
#                   # group = rep(c("x", "y", "x", "y", "z"), each = N),
#                   z = c(W1 %*% alpha_x1, W1 %*% alpha_y1,
#                         # W2 %*% alpha_x2, W2 %*% alpha_y2,
#                         W %*% alpha))
# p_layers_sim <- ggplot(dat, aes(x, y, fill=z)) +
#     geom_raster() +
#     scale_fill_viridis_c() +
#     facet_grid(layer ~ group) +
#     ggtitle("simulated layers")
#
# p_layers_sim
message("Simulated loss:", 1 / (2 * N) * sum((y - z)^2))
# Fit the model using sgd ----
# source("~/sgMRA/scripts/fit-deep-MRA-sgd.R")
# source("~/sgMRA/R/adam.R")
n_iter=10
# add in Adam optimization schedule
# profvis::profvis({
# system.time({
out <- fit_sgd(y = y, locs = locs, grid = grid,
alpha=NULL,
alpha_x1=NULL,
alpha_y1=NULL,
alpha_x2=NULL,
alpha_y2=NULL,
# alpha=alpha,
# alpha_x1=alpha_x1,
# alpha_y1=alpha_y1,
# alpha_x2=alpha_x2,
# alpha_y2=alpha_y2,
learn_rate = 0.1,
n_iter = n_iter,
n_message = 50,
penalized=TRUE,
plot_during_fit = TRUE)
MRA2 <- eval_basis(locs, grid, use_spam=use_spam)
use_spam = FALSE
MRA2 <- eval_basis(locs, grid, use_spam=use_spam)
Q2 <- make_Q(sqrt(MRA2$n_dims), phi=rep(0.9, length(MRA2$n_dims)), use_spam=use_spam)
str(Q2)
str(MRA$n_dims)
rep(0.9, length(MRA2$n_dims))
Q2 <- make_Q(sqrt(MRA2$n_dims), phi=rep(0.9, length(MRA2$n_dims)), use_spam=use_spam)
library(sgMRA)
# })
# resume the GD with last model fit
out <- fit_sgd(y = y,
locs = locs,
grid = grid,
alpha=out$alpha,
alpha_x1=out$alpha_x1,
alpha_y1=out$alpha_y1,
alpha_x2=out$alpha_x2,
alpha_y2=out$alpha_y2,
learn_rate = 0.1,
n_iter = n_iter,
n_message = 50,
penalized=TRUE,
plot_during_fit = TRUE)
# add in Adam optimization schedule
# profvis::profvis({
# system.time({
out <- fit_sgd(y = y, locs = locs, grid = grid,
alpha=NULL,
alpha_x1=NULL,
alpha_y1=NULL,
alpha_x2=NULL,
alpha_y2=NULL,
# alpha=alpha,
# alpha_x1=alpha_x1,
# alpha_y1=alpha_y1,
# alpha_x2=alpha_x2,
# alpha_y2=alpha_y2,
learn_rate = 0.1,
n_iter = n_iter,
n_message = 50,
penalized=TRUE,
plot_during_fit = TRUE)
n_iter=10
profvis::profvis({
# system.time({
out <- fit_sgd(y = y, locs = locs, grid = grid,
alpha=NULL,
alpha_x1=NULL,
alpha_y1=NULL,
alpha_x2=NULL,
alpha_y2=NULL,
# alpha=alpha,
# alpha_x1=alpha_x1,
# alpha_y1=alpha_y1,
# alpha_x2=alpha_x2,
# alpha_y2=alpha_y2,
learn_rate = 0.1,
n_iter = n_iter,
n_message = 50,
penalized=TRUE,
plot_during_fit = TRUE)
})
